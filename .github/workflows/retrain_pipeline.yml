name: Automated Model Retraining Pipeline

# Trigger the workflow when files are pushed to the data folder
on:
  push:
    paths:
      - 'data/**'
      - 'data/new_data/**'
  workflow_dispatch:  # Allow manual triggering

jobs:
  retrain-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Checkout the repository
      - name: ğŸ“¥ Checkout Repository
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Get full history for proper tracking
      
      # Step 2: Set up Python environment
      - name: ğŸ Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
          cache: 'pip'  # Cache pip dependencies for faster builds
      
      # Step 3: Install dependencies
      - name: ğŸ“¦ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      # Step 4: Create necessary directories
      - name: ğŸ“ Create Directories
        run: |
          mkdir -p models
          mkdir -p data/new_data
          mkdir -p mlruns
      
      # Step 5: Merge new data with training data
      - name: ğŸ”„ Merge New Data
        run: |
          python << EOF
          import pandas as pd
          import os
          import glob
          
          # Load existing training data
          training_data = pd.read_csv('data/training_data.csv')
          print(f"ğŸ“Š Current training data: {training_data.shape[0]} rows")
          
          # Find all new data files
          new_data_files = glob.glob('data/new_data/*.csv')
          
          if new_data_files:
              print(f"ğŸ“¥ Found {len(new_data_files)} new data file(s)")
              
              # Merge all new data
              new_data_list = []
              for file in new_data_files:
                  df = pd.read_csv(file)
                  new_data_list.append(df)
                  print(f"   - {file}: {df.shape[0]} rows")
              
              # Combine everything
              all_new_data = pd.concat(new_data_list, ignore_index=True)
              updated_training_data = pd.concat([training_data, all_new_data], ignore_index=True)
              
              # Remove duplicates if any
              updated_training_data = updated_training_data.drop_duplicates()
              
              # Save updated training data
              updated_training_data.to_csv('data/training_data.csv', index=False)
              print(f"âœ… Updated training data: {updated_training_data.shape[0]} rows")
              print(f"ğŸ“ˆ Added {updated_training_data.shape[0] - training_data.shape[0]} new rows")
          else:
              print("âš ï¸  No new data files found")
          EOF
      
      # Step 6: Train new model
      - name: ğŸ¯ Train New Model
        id: train
        run: |
          echo "ğŸš€ Starting model training..."
          python src/train.py
          
          # Extract metrics from metadata
          R2_SCORE=$(python -c "import json; print(json.load(open('models/model_metadata.json'))['metrics']['r2_score'])")
          echo "r2_score=$R2_SCORE" >> $GITHUB_OUTPUT
          echo "ğŸ“Š New Model RÂ² Score: $R2_SCORE"
      
      # Step 7: Compare models and decide deployment
      - name: ğŸ” Compare Models
        id: compare
        run: |
          # Run comparison script
          if python src/compare.py ${{ steps.train.outputs.r2_score }}; then
            echo "should_deploy=true" >> $GITHUB_OUTPUT
            echo "âœ… New model is better - will deploy"
          else
            echo "should_deploy=false" >> $GITHUB_OUTPUT
            echo "âŒ New model not better - keeping current model"
          fi
      
      # Step 8: Deploy new model (only if better)
      - name: ğŸš€ Deploy New Model
        if: steps.compare.outputs.should_deploy == 'true'
        run: |
          echo "ğŸ‰ Deploying new model..."
          
          # Configure git
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          
          # Add and commit the new model
          git add models/current_model.pkl
          git add models/model_metadata.json
          git add data/training_data.csv
          
          # Commit with detailed message
          TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M:%S UTC")
          R2_SCORE="${{ steps.train.outputs.r2_score }}"
          git commit -m "ğŸ¤– Auto-deploy: New model (RÂ²=$R2_SCORE) at $TIMESTAMP

          - Model performance improved
          - RÂ² Score: $R2_SCORE
          - Training data updated
          - Automated deployment via GitHub Actions"
          
          # Push changes
          git push
          
          echo "âœ… Model deployed successfully!"
      
      # Step 9: Keep current model (if new one isn't better)
      - name: ğŸ›‘ Keep Current Model
        if: steps.compare.outputs.should_deploy == 'false'
        run: |
          echo "âš ï¸  New model did not meet improvement threshold"
          echo "ğŸ“Œ Keeping current production model"
          echo "ğŸ’¡ Tip: Check if you need more data or better features"
      
      # Step 10: Generate summary report
      - name: ğŸ“Š Generate Summary
        if: always()
        run: |
          echo "# ğŸ¯ Model Retraining Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## ğŸ“ˆ Results" >> $GITHUB_STEP_SUMMARY
          echo "- **New Model RÂ² Score**: ${{ steps.train.outputs.r2_score }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Deployment Decision**: ${{ steps.compare.outputs.should_deploy == 'true' && 'âœ… Deployed' || 'âŒ Not Deployed' }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.compare.outputs.should_deploy }}" == "true" ]; then
            echo "## âœ… Deployment Successful" >> $GITHUB_STEP_SUMMARY
            echo "The new model has been deployed to production." >> $GITHUB_STEP_SUMMARY
          else
            echo "## â„¹ï¸ Current Model Retained" >> $GITHUB_STEP_SUMMARY
            echo "The new model did not meet improvement criteria." >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "---" >> $GITHUB_STEP_SUMMARY
          echo "*Automated by GitHub Actions* ğŸ¤–" >> $GITHUB_STEP_SUMMARY
```

**Save the file!**

---

## ğŸ“ PHASE 3: Update .gitignore (5 mins)

We need to ensure model files CAN be committed by the workflow.

### Open `.gitignore` and UPDATE it:
```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/

# MLflow
mlruns/
mlartifacts/

# Data - allow training data, ignore large files
*.csv
!data/training_data.csv
!data/new_data/*.csv

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Logs
*.log

# Allow model files (needed for deployment)
# These will be committed by GitHub Actions
!models/current_model.pkl
!models/model_metadata.json